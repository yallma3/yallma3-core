/*
* yaLLMa3 - Framework for building AI agents that are capable of learning from their environment and interacting with it.
 
 * Copyright (C) 2025 yaLLMa3
 
 * This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0.
   If a copy of the MPL was not distributed with this file, You can obtain one at https://www.mozilla.org/MPL/2.0/.
 
 * This software is distributed on an "AS IS" basis,
   WITHOUT WARRANTY OF ANY KIND, either express or implied.
   See the Mozilla Public License for the specific language governing rights and limitations under the License.
*/

import type {
  BaseNode,
  ConfigParameterType,
  NodeValue,
  NodeExecutionContext,
  NodeMetadata,
  Position,
} from "../types/types";
import { NodeRegistry } from "../NodeRegistry";

export interface ChunkingNode extends BaseNode {
  nodeType: string;
  nodeValue?: NodeValue;
  process: (context: NodeExecutionContext) => Promise<NodeValue | undefined>;
}

const metadata: NodeMetadata = {
  category: "Text",
  title: "Text Chunking",
  nodeType: "Chunking",
  nodeValue: "Chunks: 0",
  description: "Splits input text into smaller, overlapping chunks based on a maximum token count. Each chunk includes metadata such as token, character, and word counts.",
  sockets: [
    { title: "Input Text", type: "input", dataType: "string" },
    { title: "Chunks", type: "output", dataType: "json" },
  ],
  width: 380,
  height: 280,
  configParameters: [
    {
      parameterName: "Max Tokens",
      parameterType: "number",
      defaultValue: 200,
      valueSource: "UserInput",
      UIConfigurable: true,
      description: "Maximum number of tokens per chunk",
      isNodeBodyContent: false,
      i18n: {
        en: {
          "Max Tokens": {
            Name: "Max Tokens",
            Description: "Maximum number of tokens per chunk",
          },
        },
        ar: {
          "Max Tokens": {
            Name: "Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø±Ù…ÙˆØ²",
            Description: "Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ù…ÙˆØ² Ù„ÙƒÙ„ Ø¬Ø²Ø¡",
          },
        },
      },
    },
    {
      parameterName: "Overlap",
      parameterType: "number",
      defaultValue: 50,
      valueSource: "UserInput",
      UIConfigurable: true,
      description: "Number of overlapping tokens between chunks",
      isNodeBodyContent: false,
      i18n: {
        en: {
          "Overlap": {
            Name: "Overlap",
            Description: "Number of overlapping tokens between chunks",
          },
        },
        ar: {
          "Overlap": {
            Name: "Ø§Ù„ØªØ¯Ø§Ø®Ù„",
            Description: "Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡",
          },
        },
      },
    },
  ],
  i18n: {
    en: {
      category: "Text",
      title: "Text Chunking",
      nodeType: "Text Chunking",
      description: "Splits input text into smaller, overlapping chunks based on a maximum token count. Each chunk includes metadata such as token, character, and word counts.",
    },
    ar: {
      category: "Ù†Øµ",
      title: "ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ",
      nodeType: "ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ",
      description: "ÙŠÙ‚Ø³Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙØ¯Ø®Ù„ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£ØµØºØ± Ù…ØªØ¯Ø§Ø®Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ù…ÙˆØ². ÙŠØªØ¶Ù…Ù† ÙƒÙ„ Ø¬Ø²Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØµÙÙŠØ© Ù…Ø«Ù„ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ø£Ø­Ø±Ù ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª.",
    },
  },
};

// Simple tokenizer implementation for chunking
// This is a basic word-based tokenizer as a substitute for the transformer tokenizer
const simpleTokenize = (text: string): string[] => {
  // Split by whitespace and punctuation, filter out empty strings
  return text
    .split(/\s+|(?=[.,!?;:])|(?<=[.,!?;:])/)
    .filter((token) => token.trim().length > 0);
};

const simpleDetokenize = (tokens: string[]): string => {
  return tokens.join(" ").replace(/\s+([.,!?;:])/g, "$1");
};

// Token-based chunking utility function
const chunkByTokens = (
  text: string,
  maxTokens: number = 200,
  overlap: number = 50
): string[] => {
  const tokens = simpleTokenize(text);
  const chunks: string[] = [];

  for (let i = 0; i < tokens.length; i += maxTokens - overlap) {
    const chunk = tokens.slice(i, i + maxTokens);
    const decoded = simpleDetokenize(chunk);
    chunks.push(decoded);

    // If we've reached the end of tokens, break
    if (i + maxTokens >= tokens.length) {
      break;
    }
  }

  return chunks;
};

export function createChunkingNode(
  id: number,
  position: Position
): ChunkingNode {
  return {
    id,
    category: metadata.category,
    title: metadata.title,
    nodeValue: metadata.nodeValue,
    nodeType: metadata.nodeType,
    sockets: [
      {
        id: id * 100 + 1,
        title: "Input Text",
        type: "input",
        nodeId: id,
        dataType: "string",
      },
      {
        id: id * 100 + 2,
        title: "Chunks",
        type: "output",
        nodeId: id,
        dataType: "json",
      },
    ],
    x: position.x,
    y: position.y,
    width: metadata.width,
    height: metadata.height,
    selected: false,
    processing: false,
    process: async (context: NodeExecutionContext) => {
      const n = context.node as ChunkingNode;

      // Get input text
      const inputValue = await context.inputs[n.id * 100 + 1];
      const inputText = String(inputValue || "");

      if (!inputText.trim()) {
        return {
          [n.id * 100 + 2]: JSON.stringify([]),
        };
      }

      // Get configuration parameters
      const maxTokensConfig = n.getConfigParameter?.("Max Tokens");
      const overlapConfig = n.getConfigParameter?.("Overlap");

      const maxTokens = Number(maxTokensConfig?.paramValue) || 200;
      const overlap = Number(overlapConfig?.paramValue) || 50;

      console.log(
        `ðŸ”¤ Chunking text with max tokens: ${maxTokens}, overlap: ${overlap}`
      );

      // Perform chunking
      const chunks = chunkByTokens(inputText, maxTokens, overlap);

      // Create chunk objects with metadata
      const chunkObjects = chunks.map((chunk, index) => ({
        index: index,
        text: chunk,
        tokenCount: simpleTokenize(chunk).length,
        characterCount: chunk.length,
        wordCount: chunk.split(/\s+/).filter((word) => word.length > 0).length,
      }));

      // Update node value
      n.nodeValue = `Chunks: ${chunks.length}`;

      console.log(`âœ… Created ${chunks.length} chunks from input text`);

      return {
        [n.id * 100 + 2]: JSON.stringify(chunkObjects),
      };
    },
    configParameters: metadata.configParameters,
    getConfigParameters: function (): ConfigParameterType[] {
      return this.configParameters || [];
    },
    getConfigParameter(parameterName: string): ConfigParameterType | undefined {
      return (this.configParameters ?? []).find(
        (param) => param.parameterName === parameterName
      );
    },
       setConfigParameter(parameterName: string, value: string | number | boolean | undefined): void {
      const parameter = (this.configParameters ?? []).find(
        (param) => param.parameterName === parameterName
      );
      if (parameter) {
        parameter.paramValue = value;
      }
    },
  };
}

export function register(nodeRegistry: NodeRegistry): void {
  nodeRegistry.registerNodeType(
    metadata.nodeType,
    createChunkingNode,
    metadata
  );
}

// Export utility functions for external use
export { simpleTokenize, simpleDetokenize, chunkByTokens };
