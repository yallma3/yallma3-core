/*
* yaLLMa3 - Framework for building AI agents that are capable of learning from their environment and interacting with it.
 
 * Copyright (C) 2025 yaLLMa3
 
 * This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0.
   If a copy of the MPL was not distributed with this file, You can obtain one at https://www.mozilla.org/MPL/2.0/.
 
 * This software is distributed on an "AS IS" basis,
   WITHOUT WARRANTY OF ANY KIND, either express or implied.
   See the Mozilla Public License for the specific language governing rights and limitations under the License.
*/
import type {
  BaseNode,
  ConfigParameterType,
  NodeValue,
  NodeExecutionContext,
  NodeMetadata,
  Position,
} from "../types/types";
import { NodeRegistry } from "../NodeRegistry";

export interface ArxivScraperNode extends BaseNode {
  nodeType: string;
  nodeValue?: NodeValue;
  process: (context: NodeExecutionContext) => Promise<NodeValue | undefined>;
}

const metadata: NodeMetadata = {
  category: "Other",
  title: "Arxiv Scraper",
  nodeType: "ArxivScraper",
  nodeValue: "cs.AI",
  sockets: [
    { title: "Category", type: "input", dataType: "string" },
    { title: "Results", type: "output", dataType: "string" },
    { title: "Paper Count", type: "output", dataType: "number" },
  ],
  width: 340,
  height: 260,
  configParameters: [
    {
      parameterName: "Default Category",
      parameterType: "string",
      defaultValue: "cs.AI",
      valueSource: "UserInput",
      UIConfigurable: true,
      description: "Default Arxiv category to scrape",
      isNodeBodyContent: true,
    },
    {
      parameterName: "Max Results",
      parameterType: "number",
      defaultValue: 10,
      valueSource: "UserInput",
      UIConfigurable: true,
      description: "Maximum number of papers to fetch",
      isNodeBodyContent: false,
    },
  ],
};

export function createArxivScraperNode(
  id: number,
  position: Position
): ArxivScraperNode {
  return {
    id,
    category: metadata.category,
    title: metadata.title,
    nodeValue: metadata.nodeValue,
    nodeType: metadata.nodeType,
    sockets: [
      {
        id: id * 100 + 1,
        title: "Category",
        type: "input",
        nodeId: id,
        dataType: "string",
      },
      {
        id: id * 100 + 2,
        title: "Results",
        type: "output",
        nodeId: id,
        dataType: "string",
      },
      {
        id: id * 100 + 3,
        title: "Paper Count",
        type: "output",
        nodeId: id,
        dataType: "number",
      },
    ],
    x: position.x,
    y: position.y,
    width: metadata.width,
    height: metadata.height,
    selected: false,
    processing: false,
    process: async (context: NodeExecutionContext) => {
      const n = context.node as ArxivScraperNode;

      const categoryInput = await context.inputs[n.id * 100 + 1];
      const category = String(categoryInput || n.nodeValue || "cs.AI");
      const maxResults =
        Number(n.getConfigParameter?.("Max Results")?.paramValue) || 2;

      console.log(`ðŸ“¡ Scraping Arxiv category: ${category}`);

      // Local XML parser (regex-based, no DOMParser)
      function parseArxivXML(xmlText: string) {
        const papers: any[] = [];
        const entryRegex = /<entry>([\s\S]*?)<\/entry>/g;
        let match;

        while ((match = entryRegex.exec(xmlText)) !== null) {
          const entry = match[1];

          const getTag = (tag: string) => {
            const regex = new RegExp(`<${tag}>([\\s\\S]*?)<\\/${tag}>`, "i");
            const found = regex.exec(entry || "");
            return found && found[1] ? found[1].trim() : "";
          };

          const id = getTag("id");
          const title = getTag("title");
          const summary = getTag("summary");
          const published = getTag("published");

          // Extract PDF URL
          const pdfMatch = entry?.match(
            /<link[^>]+title="pdf"[^>]+href="([^"]+)"/i
          );
          const pdfUrl = pdfMatch ? pdfMatch[1] : "";

          // Format date
          let submittedDate = "";
          if (published) {
            const date = new Date(published);
            submittedDate = date.toISOString().split("T")[0] || published;
          }

          if (id && title) {
            const cleanArxivId = id.split("/").pop()?.split("v")[0] || id;
            papers.push({
              arxivId: cleanArxivId,
              version: 1,
              title,
              pdfUrl,
              submittedDate,
              abstract: summary,
            });
          }
        }

        return papers;
      }

      // Fetch now uses regex parser
      async function fetchArxivAPI(category: string, maxResults: number = 2) {
        console.log("MAXRESULTS:", maxResults);
        const apiUrl = `https://export.arxiv.org/api/query?search_query=cat:${category}&start=0&max_results=${maxResults}&sortBy=submittedDate&sortOrder=descending`;

        console.log(`Fetching from ArXiv API: ${apiUrl}`);

        try {
          const response = await fetch(apiUrl, {
            method: "GET",
            headers: {
              "User-Agent": "Mozilla/5.0 (compatible; ArxivScraper/1.0)",
            },
          });

          if (!response.ok) {
            throw new Error(
              `API request failed: ${response.status} ${response.statusText}`
            );
          }

          const xmlText = await response.text();
          console.log(`Received XML response, length: ${xmlText.length}`);

          // Use safe regex parser instead of DOMParser
          const papers = parseArxivXML(xmlText);
          console.log(`Found ${papers.length} entries in XML`);

          return papers;
        } catch (error) {
          console.error("ArXiv API fetch failed:", error);
          console.log("Returning mock data for testing...");
          return [
            {
              arxivId: "2501.00001",
              version: 1,
              title: "Sample AI Paper Title",
              pdfUrl: "https://arxiv.org/pdf/2501.00001.pdf",
              submittedDate: "2025-01-01",
              abstract: "This is a sample abstract for testing purposes.",
            },
          ];
        }
      }

      try {
        // Use ArXiv API instead of web scraping to avoid CORS issues
        const papers = await fetchArxivAPI(category, maxResults);

        console.log(` Found ${papers.length} papers in ${category}`);

        return {
          [n.id * 100 + 2]: JSON.stringify(papers),
          [n.id * 100 + 3]: papers.length,
        };
      } catch (err) {
        console.error(" Arxiv Scraper Error:", err);
        return {
          [n.id * 100 + 2]: JSON.stringify([]),
          [n.id * 100 + 3]: 0,
        };
      }
    },
    configParameters: metadata.configParameters,
    getConfigParameters: function (): ConfigParameterType[] {
      return this.configParameters || [];
    },
    getConfigParameter(parameterName: string): ConfigParameterType | undefined {
      return (this.configParameters ?? []).find(
        (param) => param.parameterName === parameterName
      );
    },
    setConfigParameter(parameterName, value): void {
      const parameter = (this.configParameters ?? []).find(
        (param) => param.parameterName === parameterName
      );
      if (parameter) {
        parameter.paramValue = value;
      }
    },
  };
}

export function register(
  nodeRegistry: NodeRegistry,
  category: string = "Tools"
): void {
  console.log(
    "Registering Arxiv Scraper Node (API-based) under category:",
    category
  );
  nodeRegistry.registerNodeType(
    "ArxivScraper",
    createArxivScraperNode,
    metadata
  );
}
